{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "from tensorflow.compat.v1 import set_random_seed\n",
    "set_random_seed(2)\n",
    "seed(1)\n",
    "\n",
    "from tcn import TCN\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Input, Model\n",
    "from pathlib import Path\n",
    "\n",
    "import dask.array as da\n",
    "import zarr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import robust_scale\n",
    "from sklearn.metrics import fbeta_score\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using fbeta because we want more False Positives than False Negatives\n",
    "def fbeta(y_true, y_pred, threshold_shift=0):\n",
    "    beta = 2\n",
    "\n",
    "    # just in case of hipster activation at the final layer\n",
    "    y_pred = K.clip(y_pred, 0, 1)\n",
    "\n",
    "    # shifting the prediction threshold from .5 if needed\n",
    "    y_pred_bin = K.round(y_pred + threshold_shift)\n",
    "\n",
    "    tp = K.sum(K.round(y_true * y_pred_bin)) + K.epsilon()\n",
    "    fp = K.sum(K.round(K.clip(y_pred_bin - y_true, 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip(y_true - y_pred, 0, 1)))\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    beta_squared = beta ** 2\n",
    "    return (beta_squared + 1) * (precision * recall) / (beta_squared * precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = Path(\"..\", \"data\", \"interim\")\n",
    "train_dask = da.from_zarr(str(data_folder.joinpath(\"breath_data_train.zarr\")))\n",
    "train = train_dask.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The TCN package expected 3 axes, even if it's 2D, so I added an extra\n",
    "X = train[:,:-2][:, :, np.newaxis]\n",
    "\n",
    "y = train[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 515 samples, validate on 129 samples\n",
      "Epoch 1/10\n",
      "515/515 [==============================] - 14s 26ms/sample - loss: 0.3827 - fbeta: 0.9070 - val_loss: 0.1485 - val_fbeta: 0.9538\n",
      "Epoch 2/10\n",
      "515/515 [==============================] - 12s 24ms/sample - loss: 0.2692 - fbeta: 0.9147 - val_loss: 0.2612 - val_fbeta: 0.9538\n",
      "Epoch 3/10\n",
      "515/515 [==============================] - 12s 23ms/sample - loss: 0.2723 - fbeta: 0.9147 - val_loss: 0.1243 - val_fbeta: 0.9538\n",
      "Epoch 4/10\n",
      "515/515 [==============================] - 12s 24ms/sample - loss: 0.2800 - fbeta: 0.9186 - val_loss: 0.2213 - val_fbeta: 0.9538\n",
      "Epoch 5/10\n",
      "515/515 [==============================] - 12s 24ms/sample - loss: 0.3268 - fbeta: 0.9186 - val_loss: 0.2169 - val_fbeta: 0.9538\n",
      "Epoch 6/10\n",
      "515/515 [==============================] - 12s 24ms/sample - loss: 0.2675 - fbeta: 0.9147 - val_loss: 0.2341 - val_fbeta: 0.9538\n",
      "Epoch 7/10\n",
      "515/515 [==============================] - 12s 24ms/sample - loss: 0.2670 - fbeta: 0.9070 - val_loss: 0.1994 - val_fbeta: 0.9538\n",
      "Epoch 8/10\n",
      "515/515 [==============================] - 12s 24ms/sample - loss: 0.2040 - fbeta: 0.9218 - val_loss: 0.3897 - val_fbeta: 0.9538\n",
      "Epoch 9/10\n",
      "515/515 [==============================] - 12s 24ms/sample - loss: 0.2339 - fbeta: 0.9208 - val_loss: 0.1361 - val_fbeta: 0.9538\n",
      "Epoch 10/10\n",
      "515/515 [==============================] - 12s 23ms/sample - loss: 0.3018 - fbeta: 0.9109 - val_loss: 0.4208 - val_fbeta: 0.7538\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential(\n",
    "    [\n",
    "        TCN(\n",
    "            input_shape=[7500, 1],\n",
    "            kernel_size=2,\n",
    "            activation=\"relu\",\n",
    "            dilations=[rate for rate in (1, 2, 4, 8) * 2],\n",
    "            return_sequences=False,\n",
    "        ),\n",
    "        Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[fbeta],\n",
    ")\n",
    "history = model.fit(X, classifier_y, epochs=10, batch_size=2, validation_split=0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
